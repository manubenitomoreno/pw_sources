{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.wkt import loads\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.font_manager\n",
    "import seaborn as sn\n",
    "from scipy.stats import zscore\n",
    "import numpy as np\n",
    "#matplotlib.font_manager.findSystemFonts(fontpaths=None, fontext='ttf')\n",
    "sn.set_style(\"white\")\n",
    "\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "csfont = {'fontname':'Adobe Garamond Pro','fontsize':30}\n",
    "hfont = {'fontname':'Adobe Garamond Pro','fontsize':12}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = pd.read_csv(r\"G:\\Mi unidad\\walknet_datalake\\extractions\\pois_other_300.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266994\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r\"G:\\Mi unidad\\walknet_datalake\\sources\\edm2018\\level1\\level1_edm2018.csv\",sep=\",\")\n",
    "print(len(df))\n",
    "df = df[df['ZT1259'].str.startswith((\"005\", \"006\", \"007\", \"022\", \"026\", \"045\", \"049\", \"058\", \"065\", \"073\", \"074\", \"080\", \"084\", \"092\", \"104\", \"106\", \"113\", \"115\", \"127\", \"130\", \"134\", \"148\", \"167\", \"176\", \"177\", \"181\", \"184\", \"079\",\"123\",\"903\"))]\n",
    "df['CODMUNI'] = df['CODMUNI'].astype(str).str.zfill(3)\n",
    "df['edge_id'] = df['VORIZT1259']+\"|\"+df['VDESZT1259']\n",
    "relations = df['edge_id'].unique()\n",
    "df.drop_duplicates(subset=['ID_HOGAR','ID_IND','ID_VIAJE'],inplace=True)\n",
    "df = df[df['VDES']!='Casa']\n",
    "df = df[df['VORI']=='Casa']\n",
    "df = df[df['C8ACTIV']!='Otra situación']\n",
    "df = df[df['CPMR'] == 'No']\n",
    "#df = df[df['EMODO'].isin(['Andando/ pie','Coche pasajero','Coche conductor'])]\n",
    "df = df[~df['VFRECUENCIA'].isin(['Alguna vez','Es la primera vez que hago el viaje'])]\n",
    "df = df[df['MOTIVO_PRIORITARIO'].isin(['Compras','Estudio', 'Ocio', 'Deporte/ dar un paseo','Acompañamiento a otra persona','Médico'])]\n",
    "df = df[df['DISTANCIA_VIAJE'] < 900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['VORIHORAINI'] = df['VORIHORAINI'].astype(int).astype(str).str.zfill(4).str[:2] + \":\" + df['VORIHORAINI'].astype(int).astype(str).str.zfill(4).str[2:]\n",
    "df['VDESHORAFIN'] = df['VDESHORAFIN'].astype(int).astype(str).str.zfill(4).str[:2] + \":\" + df['VDESHORAFIN'].astype(int).astype(str).str.zfill(4).str[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['VDESHORAFIN'].str.startswith((\"25\",\"26\"))]\n",
    "df = df[~df['VORIHORAINI'].str.startswith((\"25\",\"26\"))]\n",
    "\n",
    "df.loc[df['VORIHORAINI'].str.startswith(\"24\"),'VORIHORAINI'] = df[df['VORIHORAINI'].str.startswith(\"24\")]['VORIHORAINI'].str.replace(\"24:\",\"00:\")\n",
    "df.loc[df['VDESHORAFIN'].str.startswith(\"24\"),'VDESHORAFIN'] = df[df['VDESHORAFIN'].str.startswith(\"24\")]['VDESHORAFIN'].str.replace(\"24:\",\"00:\")\n",
    "\n",
    "df['VDESHORAFIN'] = pd.to_datetime(df['VDESHORAFIN'], format= '%H:%M' )\n",
    "df['VORIHORAINI'] = pd.to_datetime(df['VORIHORAINI'], format= '%H:%M' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\katul\\AppData\\Local\\Temp\\ipykernel_4428\\1053556119.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['DURACION_VIAJE'].fillna(0,inplace=True)\n",
      "C:\\Users\\katul\\AppData\\Local\\Temp\\ipykernel_4428\\1053556119.py:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['VELOCIDAD_VIAJE'].fillna(0,inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df['DURACION_VIAJE'] = ((df['VDESHORAFIN'] - df['VORIHORAINI']).dt.seconds)/60\n",
    "\n",
    "df['VELOCIDAD_VIAJE'] = df['DISTANCIA_VIAJE']/(df['DURACION_VIAJE']/60)\n",
    "\n",
    "df['DURACION_VIAJE'].fillna(0,inplace=True)\n",
    "df['VELOCIDAD_VIAJE'].fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['VDESHORAFIN'] = df['VDESHORAFIN'].dt.hour\n",
    "df['VORIHORAINI'] = df['VORIHORAINI'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = {\n",
    "    'C2SEXO':('Gender',{'Hombre':'Male','Mujer':'Female'},(0,0)),\n",
    "    'C4NAC':('Foreign',{'No':'No','Si':'Yes'},(0,1)),\n",
    "    'CPMR':('PRM',{'No':'No','Si':'Yes'},(1,0)),\n",
    "    'C13TARJETA':('TravelCard',{'Tarjeta de Transporte Público TTP':\"Yes\", 'Ninguna':\"No\", 'Ambas tarjetas':\"Yes\",'Tarjeta MULTI':\"Yes\"},(1,1)),\n",
    "    'VVEHICULO':('HasVehicle',{'Si':'Yes','No':'No'},(2,0)),\n",
    "    'C6CARNE':('HasLicense',{'No tiene':\"No\", 'Carné de coche (B) o superior  (C, D ó E)':\"Yes\",'Carné de moto y coche':\"Yes\", 'Carné de moto A1/A2':\"Yes\", 'Sólo licencia de ciclomotor':\"Yes\"},(2,1)),\n",
    "    'VFRECUENCIA':('Frequency',{'A diario, de lunes a viernes':\"Daily\",'Entre 2 y 4 días laborables por semana':\"Frequently\",'Es la primera vez que hago el viaje':\"0\",'Menos de dos días laborables por semana':\"Less Frequently\", 'Alguna vez':\"Sometimes\"},(3,0)),\n",
    "    'DIASEM':('DayOfWeek',{\"Lunes\":\"Monday\",\"Martes\":\"Tuesday\",\"Miércoles\":\"Wednesday\",\"Jueves\":\"Thursday\"},(3,1)),\n",
    "    'C7ESTUD':('EducationLevel',{'Primera etapa de Educación Secundaria y similar':\"Second - A\",'Educación Primaria':\"First\",'Segunda etapa de Educación Secundaria y similar':\"Second - B\",'Menos que Primaria':\"No\",'Enseñanzas de Formación Profesional de grado superior y equivalentes':\"Third\",'Educación postsecundaria no superior':\"Second - B\", 'Grados universitarios/licenciaturas/másteres y enseñanzas de doctorado':\"Third\"},(4,0)),\n",
    "    'C8ACTIV':('Activity',{'Parado, ha trabajado antes':\"Unemployed\", 'Jubilado / Retirado/ Pensionista':\"Retired\",'Estudiante':\"Student\", 'Trabaja':\"Worker\", 'Trabaja y estudia':\"Worker\",'Cuidado de familiares':\"Caretaker\", 'Trabajo doméstico no remunerado':\"Caretaker\", 'Parado, busca primer trabajo':\"Unemployed\"},(4,1)),\n",
    "    'VNOPRIVADO':(\"NoPrivate\",{'No me gusta el coche':\"Dislike\", 'Más incómodo':\"Less Convenient\",'Evitar el atasco':\"Congestion\",'Contamino menos':\"Other\",'Dificultad de aparcamiento':\"Hard to Park\",'Otros':\"Other\",'Tardo más':\"Takes Longer\",'Es más caro':\"Other\"},(5,0)),\n",
    "    'VNOPUBLICO':(\"NoPublic\",{'No hay servicio público':\"Bad Service\",'Es más caro':\"Other\",'Tardo más':\"Takes Longer\",'El destino está muy cerca':\"Very Close\",'Otros':\"Other\",'Necesito mi vehículo para trabajar o gestión personal':\"Other\",'Mala combinación del tte.público':\"Bad Service\",'Prefiero ir andando / en bicicleta':\"Prefer Active Modes\",'Mi situación personal condiciona esta elección modal':\"Bad Service\",'Más incómodo':\"Less Convenient\", 'Por desconocimiento/falta de información':\"Other\",'No me gusta el tte. público':\"Other\"},(5,1)),\n",
    "    \"MOTIVO_PRIORITARIO\":(\"TripPurpose\",{'Compras':\"Shopping\",'Ocio':\"Leisure\",'Estudio':\"Study\",'Trabajo':\"Work\",'Deporte/ dar un paseo':\"Sport / Stroll\",'Otro domicilio':\"Other\",'Asunto personal':\"Other\",'Acompañamiento a otra persona':\"Care\",'Médico':\"Care\",'Gestión de trabajo':\"Work\",'Otros':\"Other\"},(6,0)),\n",
    "    \"MODO_PRIORITARIO\":(\"TransportMode\",{'Coche conductor particular' :\"Motor - Personal\",'Coche pasajero particular':\"Motor - Personal\",'Moto/ciclomotor particular':\"Motor - Personal\",'Andando/pie' : \"Walking\",'Autobus urbano Madrid EMT':\"Bus\",'Autobus interurbano':\"Bus\",'Autobus discrecional':\"Bus\",'Autobus de largo recorrido':\"Bus\",'Autobus urbano otro municipio':\"Bus\",'Otros':\"Other\",'Metro':\"Train\",'Renfe Cercanías':\"Train\",'Resto renfe':\"Train\",'Metro ligero/tranvía':\"Train\", 'Coche conductor empresa' : \"Motor - Shared\",'Coche pasajero empresa': \"Motor - Shared\",'Taxi':\"Motor - Shared\",'Coche pasajero alquiler con conductor': \"Motor - Shared\",'Coche conductor alquiler sin conductor': \"Motor - Shared\",'Moto/ciclomotor empresa': \"Motor - Shared\",'Moto/ciclomotor publica': \"Motor - Shared\",'Bicicleta particular':\"Cycling\",'Bicicleta publica':\"Cycling\",'Bicicleta empresa':\"Cycling\"},(6,1))\n",
    "    }\n",
    "\n",
    "for c,i in categorical.items():\n",
    "\n",
    "    df[c] = df[c].map(i[1])\n",
    "    df.rename(columns={c:i[0]},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['VELOCIDAD_VIAJE'] < 80]\n",
    "\n",
    "df = df[df['DURACION_VIAJE'] < 300]\n",
    "df = df[df['DURACION_VIAJE'] >= 0]\n",
    "\n",
    "df = df[df['N_VIAJES_POR_HOGAR'] < 20]\n",
    "\n",
    "df = df[df['DISTANCIA_VIAJE'] < 20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns = {'DISTANCIA_VIAJE':'TripDistance'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous = {\n",
    "    'EDAD_FIN':('Age',(0,0)),\n",
    "    'N_MIEMBROS_POR_HOGAR':('HouseholdSize',(0,1)),\n",
    "    'N_VIAJES_POR_HOGAR':('TripsHousehold',(1,0)),\n",
    "    'N_ETAPAS_POR_VIAJE':('TripStages',(1,1)),\n",
    "    'VORIHORAINI':('StartTime',(2,0)),\n",
    "    'VDESHORAFIN':('EndTime',(2,1)),\n",
    "    'DURACION_VIAJE':('TripDuration',(3,0)),\n",
    "    'VELOCIDAD_VIAJE':('TripSpeed',(3,1))\n",
    "    }\n",
    "\n",
    "for c,i in continuous.items():\n",
    "\n",
    "    df.rename(columns={c:i[0]},inplace=True)\n",
    "    df[i[0]] = df[i[0]].astype(float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['Age']<5,'Age Status'] = \"Less than 5\"\n",
    "df.loc[(df['Age']>=5) & (df['Age']<12),'Age Status'] = \"Ages 6-12\"\n",
    "df.loc[(df['Age']>=12) & (df['Age']<18),'Age Status'] = \"Ages 13-18\"\n",
    "df.loc[(df['Age']>=18) & (df['Age']<40),'Age Status'] = \"Ages 19-40\"\n",
    "df.loc[(df['Age']>=40) & (df['Age']<65),'Age Status'] = \"Ages 41-65\"\n",
    "df.loc[df['Age']>=65,'Age Status'] = \"More than 65\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hd = {}\n",
    "h15 = {}\n",
    "h30 = {}\n",
    "for h in list(df['ID_HOGAR'].unique()):\n",
    "    dist = list(df[df['ID_HOGAR'] == h]['TripDuration'].values)\n",
    "    \n",
    "    if all(di < 30 for di in dist): h30.update({h:1})\n",
    "    else: h30.update({h:0})\n",
    "    \n",
    "    if all(di < 15 for di in dist): h15.update({h:1})\n",
    "    else: h15.update({h:0})\n",
    "\n",
    "    hd.update({h:'Other'})\n",
    "    d = df[df['ID_HOGAR'] == h][['ID_IND','Age','Age Status','HouseholdSize','Activity']].drop_duplicates(subset='ID_IND')\n",
    "    if d['HouseholdSize'].values[0] == 1:\n",
    "        if d['Activity'].values[0] == 'Retired': hd.update({h:'Retirees'})\n",
    "        elif d['Activity'].values[0] == 'Student': hd.update({h:'Students (age 19+)'})\n",
    "        elif d['Activity'].values[0] in ['Worker']: hd.update({h:'Full-Time Workers'})\n",
    "        else: hd.update({h:'Other'})\n",
    "    else:\n",
    "        if any(act in ['Retired'] for act in d['Activity'].values): hd.update({h:'Retirees'})\n",
    "        if any(act in ['Worker'] for act in d['Activity'].values): hd.update({h:'Full-Time Workers'})\n",
    "        if any(age == \"Ages 13-18\" for age in d['Age Status'].values): hd.update({h:'Students (Age 13–18)'})\n",
    "        if any(age == \"Ages 6-12\" for age in d['Age Status'].values): hd.update({h:'Students (Age 6–12)'})\n",
    "        if any(age == \"Less than 5\" for age in d['Age Status'].values): hd.update({h:'Children'})\n",
    "        \n",
    "df['HouseholdStructure'] = df['ID_HOGAR'].map(hd)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Trip15'] = \"\"\n",
    "df.loc[df['TripDuration']<=15,'Trip15'] = 'Yes'\n",
    "df.loc[df['TripDuration']>15,'Trip15'] = 'No'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['ID_ETAPA','Foreign', 'C5CAM','C9PROF', 'C10SECTOR','C14ABONO', 'DDIA','DMES', 'DANNO', 'DayOfWeek', 'DNOVIAJO','C11ZT1259', 'C12ZT1259','PRM', 'TIPO_ENCUESTA','NOMMUNI', 'CODPROV', 'NOMPROV',\n",
    "       'ZT1259', 'CZ208', 'ELE_HOGAR_NUEVO', 'A1PER', 'A2PER4', 'B1NVE',\n",
    "       'V1B11TIPO', 'V1B12CARB', 'V1B13EST', 'V2B11TIPO1', 'V2B12CARB1',\n",
    "       'V2B13EST1', 'V3B11TIPO1', 'V3B12CARB1', 'V3B13EST1', 'V4B11TIPO1',\n",
    "       'V4B12CARB1', 'V4B13EST1', 'V5B11TIPO1', 'V5B12CARB1', 'V5B13EST1','EMODO', 'EMODO1','MODO', 'ELINEAEMPRESA_ORIGINAL', 'ESUBIDA', 'ESUBIDA_cod', 'EBAJADA',\n",
    "       'EBAJADA_cod', 'ETITULO', 'EESTACIONA', 'EOCUPACION', 'ETDESPH',\n",
    "       'COD_MUN_PARADA', 'ELE_G_POND_Esc2', 'VORI','VDES',\n",
    "       'EndTime', 'Frequency',  'VDESZT1259', 'ELE_G_POND_ESC2', 'edge_id', 'TripSpeed','Trip15'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "map = {'CODMUNI':'id_municipality', 'VORIZT1259':'id_taz','ELE_G_POND':'elevator',  'ID_HOGAR':'id_household','ID_IND':'id_person', 'ID_VIAJE':'id_trip',\n",
    "   'TripPurpose':'trip_purpose', 'TripDistance':'trip_distance','TripStages':'trip_stages', 'TripDuration':'trip_duration', 'TransportMode':'trip_mode',\n",
    "   'Gender':'dem_gender', 'Age':'dem_age','Age Status':'dem_cohort','EducationLevel':'dem_education', 'Activity':'dem_activity',\n",
    "   'HouseholdSize':'dem_hou_size','HouseholdStructure':'dem_hou_structure','TripsHousehold':'dem_hou_trips',\n",
    "   'HasLicense':'dem_att_license', 'HasVehicle':'dem_att_vehicle',\n",
    "   'TravelCard':'dem_att_card', 'NoPrivate':'dem_att_noprivate', 'NoPublic':'dem_att_nopublic'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[map.keys()].rename(columns=map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['id'] = df['id_household'].astype(int).astype(str)+df['id_person'].astype(int).astype(str)+df['id_trip'].astype(int).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "r\"G:\\Mi unidad\\walknet_datalake\\extractions\\pois_network_centrality_300.csv\",\n",
    "r\"G:\\Mi unidad\\walknet_datalake\\extractions\\pois_network_centrality_600.csv\",\n",
    "r\"G:\\Mi unidad\\walknet_datalake\\extractions\\pois_network_centrality_900.csv\",\n",
    "r\"G:\\Mi unidad\\walknet_datalake\\extractions\\pois_network_centrality_1200.csv\",\n",
    "r\"G:\\Mi unidad\\walknet_datalake\\extractions\\pois_network_centrality_1500.csv\",\n",
    "r\"G:\\Mi unidad\\walknet_datalake\\extractions\\pois_design_300.csv\",\n",
    "r\"G:\\Mi unidad\\walknet_datalake\\extractions\\pois_design_600.csv\",\n",
    "r\"G:\\Mi unidad\\walknet_datalake\\extractions\\pois_design_900.csv\",\n",
    "r\"G:\\Mi unidad\\walknet_datalake\\extractions\\pois_design_1200.csv\",\n",
    "r\"G:\\Mi unidad\\walknet_datalake\\extractions\\pois_design_1500.csv\",\n",
    "r\"G:\\Mi unidad\\walknet_datalake\\extractions\\pois_other_300.csv\",\n",
    "r\"G:\\Mi unidad\\walknet_datalake\\extractions\\pois_other_600.csv\",\n",
    "r\"G:\\Mi unidad\\walknet_datalake\\extractions\\pois_other_900.csv\",\n",
    "r\"G:\\Mi unidad\\walknet_datalake\\extractions\\pois_other_1200.csv\",\n",
    "r\"G:\\Mi unidad\\walknet_datalake\\extractions\\pois_other_1500.csv\",\n",
    "r\"G:\\Mi unidad\\walknet_datalake\\extractions\\pois_walkable_trips_300.csv\",\n",
    "r\"G:\\Mi unidad\\walknet_datalake\\extractions\\pois_walkable_trips_600.csv\",\n",
    "r\"G:\\Mi unidad\\walknet_datalake\\extractions\\pois_walkable_trips_900.csv\",\n",
    "r\"G:\\Mi unidad\\walknet_datalake\\extractions\\pois_walkable_trips_1200.csv\",\n",
    "r\"G:\\Mi unidad\\walknet_datalake\\extractions\\pois_walkable_trips_1500.csv\",\n",
    "r\"G:\\Mi unidad\\walknet_datalake\\extractions\\pois_transacc_300.csv\",\n",
    "r\"G:\\Mi unidad\\walknet_datalake\\extractions\\pois_transacc_600.csv\",\n",
    "r\"G:\\Mi unidad\\walknet_datalake\\extractions\\pois_transacc_900.csv\",\n",
    "r\"G:\\Mi unidad\\walknet_datalake\\extractions\\pois_transacc_1200.csv\",\n",
    "r\"G:\\Mi unidad\\walknet_datalake\\extractions\\pois_transacc_1500.csv\",\n",
    "r\"G:\\Mi unidad\\walknet_datalake\\extractions\\pois_parkacc_300.csv\",\n",
    "r\"G:\\Mi unidad\\walknet_datalake\\extractions\\pois_parkacc_600.csv\",\n",
    "r\"G:\\Mi unidad\\walknet_datalake\\extractions\\pois_parkacc_900.csv\",\n",
    "r\"G:\\Mi unidad\\walknet_datalake\\extractions\\pois_parkacc_1200.csv\",\n",
    "r\"G:\\Mi unidad\\walknet_datalake\\extractions\\pois_parkacc_1500.csv\",\n",
    "r\"G:\\Mi unidad\\walknet_datalake\\extractions\\pois_far_localacc_300.csv\",\n",
    "r\"G:\\Mi unidad\\walknet_datalake\\extractions\\pois_far_localacc_600.csv\",\n",
    "r\"G:\\Mi unidad\\walknet_datalake\\extractions\\pois_far_localacc_900.csv\",\n",
    "r\"G:\\Mi unidad\\walknet_datalake\\extractions\\pois_far_localacc_1200.csv\",\n",
    "r\"G:\\Mi unidad\\walknet_datalake\\extractions\\pois_far_localacc_1500.csv\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {d.split(\"\\\\\")[-1].split(\".\")[0]:pd.read_csv(d) for d in datasets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tz_id</th>\n",
       "      <th>poi_id</th>\n",
       "      <th>des_mean_degree</th>\n",
       "      <th>des_straightness</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>079-09-204</td>\n",
       "      <td>28900-1-6659</td>\n",
       "      <td>2.665272</td>\n",
       "      <td>0.7796</td>\n",
       "      <td>0101000000A8C64BB755881A4177BE9FAAFD145141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>079-09-204</td>\n",
       "      <td>28900-1-6661</td>\n",
       "      <td>2.665272</td>\n",
       "      <td>0.7796</td>\n",
       "      <td>0101000000B29DEF2755881A4183C0CA5DF7145141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>079-09-204</td>\n",
       "      <td>28900-1-6680</td>\n",
       "      <td>2.665272</td>\n",
       "      <td>0.7796</td>\n",
       "      <td>01010000006ABC741368881A411D5A6417FB145141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>079-09-204</td>\n",
       "      <td>28900-1-6660</td>\n",
       "      <td>2.665272</td>\n",
       "      <td>0.7796</td>\n",
       "      <td>0101000000A8C64BB755881A41FED47895FB145141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>079-09-204</td>\n",
       "      <td>28900-1-6677</td>\n",
       "      <td>2.665272</td>\n",
       "      <td>0.7796</td>\n",
       "      <td>01010000002DB29D6F65881A41DF4F8D13FE145141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309072</th>\n",
       "      <td>176-003</td>\n",
       "      <td>28176-1-5574</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>01010000007368916D47A719419CC42078EF165141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309073</th>\n",
       "      <td>006-009</td>\n",
       "      <td>28006-1-1593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>01010000000C022B07522E1B41A69BC4C4F11E5141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309074</th>\n",
       "      <td>006-009</td>\n",
       "      <td>28006-1-1608</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0101000000E3A59B44692E1B419CC42044F01E5141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309075</th>\n",
       "      <td>079-16-443</td>\n",
       "      <td>28900-1-89070</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>010100000023DBF93EDE1A1B41AE47E152BC175141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309076</th>\n",
       "      <td>079-16-443</td>\n",
       "      <td>28900-1-88928</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0101000000E3A59BC4811A1B41CDCCCC34BD175141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>309077 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             tz_id         poi_id  des_mean_degree  des_straightness  \\\n",
       "0       079-09-204   28900-1-6659         2.665272            0.7796   \n",
       "1       079-09-204   28900-1-6661         2.665272            0.7796   \n",
       "2       079-09-204   28900-1-6680         2.665272            0.7796   \n",
       "3       079-09-204   28900-1-6660         2.665272            0.7796   \n",
       "4       079-09-204   28900-1-6677         2.665272            0.7796   \n",
       "...            ...            ...              ...               ...   \n",
       "309072     176-003   28176-1-5574              NaN               NaN   \n",
       "309073     006-009   28006-1-1593              NaN               NaN   \n",
       "309074     006-009   28006-1-1608              NaN               NaN   \n",
       "309075  079-16-443  28900-1-89070              NaN               NaN   \n",
       "309076  079-16-443  28900-1-88928              NaN               NaN   \n",
       "\n",
       "                                          geometry  \n",
       "0       0101000000A8C64BB755881A4177BE9FAAFD145141  \n",
       "1       0101000000B29DEF2755881A4183C0CA5DF7145141  \n",
       "2       01010000006ABC741368881A411D5A6417FB145141  \n",
       "3       0101000000A8C64BB755881A41FED47895FB145141  \n",
       "4       01010000002DB29D6F65881A41DF4F8D13FE145141  \n",
       "...                                            ...  \n",
       "309072  01010000007368916D47A719419CC42078EF165141  \n",
       "309073  01010000000C022B07522E1B41A69BC4C4F11E5141  \n",
       "309074  0101000000E3A59B44692E1B419CC42044F01E5141  \n",
       "309075  010100000023DBF93EDE1A1B41AE47E152BC175141  \n",
       "309076  0101000000E3A59BC4811A1B41CDCCCC34BD175141  \n",
       "\n",
       "[309077 rows x 5 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['pois_network_centrality_900']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in data.items():\n",
    "    data[k] = v[['poi_id']+[c for c in v.columns if not c in ['poi_id','tz_id','local_population','geometry']]]\n",
    "    #.set_index('poi_id')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in data.items():\n",
    "    if 'pois_parkacc' in k:\n",
    "        data[k] = pd.concat([v[v['park_type'] == t].set_index('poi_id')['unique_parks_count'].rename(f'acc_parks_{t}') for t in [\"S\",\"M\",\"L\"]],axis=1)\n",
    "    else: data[k] = v.set_index('poi_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['pois_other_300'][data['pois_other_300']['des_building_age'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = pd.read_csv(r\"G:\\Mi unidad\\walknet_datalake\\extractions\\fn_pois_template.csv\").set_index('poi_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "template['geometry'] = template['geometry'].apply(loads)\n",
    "template = gpd.GeoDataFrame(template, geometry='geometry', crs= 25830)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['tz_id', 'node_id', 'local_population', \n",
    "\n",
    "'dem_income', 'dem_household_size', 'dem_mean_age',\n",
    "'dem_population_18', 'dem_population_65',\n",
    "\n",
    "'acc_care_other','acc_care_public',\n",
    "'acc_school_superior', 'acc_school_basic',\n",
    "'acc_leisure_bar', 'acc_leisure_cultural', 'acc_leisure_shows',\n",
    "'acc_shopping_mall', 'acc_shopping_market', 'acc_shopping_alone',\n",
    "'acc_sport_other',\n",
    "'acc_transportation',\n",
    "'acc_parks_S', 'acc_parks_M', 'acc_parks_L',\n",
    "'dens_pop_total', 'dens_hou_total',\n",
    "'dens_far', 'dens_far_ag',\n",
    "'den_perc_unbuilt', 'dens_built_total',\n",
    "'den_perc_housing_sfr','den_perc_housing_ch',\n",
    "'den_perc_care_other', 'den_perc_care_public',\n",
    "'den_perc_school_superior', 'den_perc_school_basic',\n",
    "'den_perc_leisure_bar', 'den_perc_leisure_cultural',\n",
    "'den_perc_leisure_shows', 'den_perc_shopping_mall',\n",
    "'den_perc_shopping_market', 'den_perc_shopping_alone',\n",
    "'den_perc_sport_other', 'den_perc_office', 'den_perc_industrial',\n",
    "'den_perc_storage', 'den_perc_parking', 'den_perc_hotel',\n",
    "'den_perc_religious', 'den_perc_infra', \n",
    "\n",
    "'div_wt_care', 'div_ut_care', 'div_wt_school', 'div_ut_school',\n",
    "'div_wt_leisure', 'div_ut_leisure', 'div_wt_shopping',\n",
    "'div_ut_shopping', 'div_wt_sport', 'div_ut_sport', \n",
    "\n",
    "'des_mean_degree', 'des_straightness', #'des_betweenness',\n",
    "'des_total_length','des_block_length', 'des_culdesac', 'des_slope','des_building_age',\n",
    "       \n",
    "'geometry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "btw = gpd.read_file(r\"G:\\Mi unidad\\walknet_datalake\\temp\\betweenness.gpkg\")[['degree','closeness','betweenness','eigenvector','geometry']].rename(columns ={'degree':'des_degree','closeness':'des_closeness','betweenness':'des_betweenness','eigenvector':'des_eigenvector'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_300 = pd.merge(\n",
    "    template,\n",
    "    pd.concat([v for k,v in data.items() if \"300\" in k],axis=1),\n",
    "    left_index=True,right_index=True, how='left')[cols]\n",
    "df_600 = pd.merge(\n",
    "    template,\n",
    "    pd.concat([v for k,v in data.items() if \"600\" in k],axis=1),\n",
    "    left_index=True,right_index=True, how='left')[cols]\n",
    "df_900 = pd.merge(\n",
    "    template,\n",
    "    pd.concat([v for k,v in data.items() if \"900\" in k],axis=1),\n",
    "    left_index=True,right_index=True, how='left')[cols]\n",
    "df_1200 = pd.merge(\n",
    "    template,\n",
    "    pd.concat([v for k,v in data.items() if \"1200\" in k],axis=1),\n",
    "    left_index=True,right_index=True, how='left')[cols]\n",
    "df_1500 = pd.merge(\n",
    "    template,\n",
    "    pd.concat([v for k,v in data.items() if \"1500\" in k],axis=1),\n",
    "    left_index=True,right_index=True, how='left')[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_300 = df_300[df_300['local_population'] > 0]\n",
    "df_600 = df_600[df_600['local_population'] > 0]\n",
    "df_900 = df_900[df_900['local_population'] > 0]\n",
    "df_1200 = df_1200[df_1200['local_population'] > 0]\n",
    "df_1500 = df_1500[df_1500['local_population'] > 0]\n",
    "\n",
    "df_300 = df_300[df_300['des_building_age'] > 1880]\n",
    "df_600 = df_600[df_600['des_building_age'] > 1880]\n",
    "df_900 = df_900[df_900['des_building_age'] > 1880]\n",
    "df_1200 = df_1200[df_1200['des_building_age'] > 1880]\n",
    "df_1500 = df_1500[df_1500['des_building_age'] > 1880]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_by_nearest(gdf1,gdf2):\n",
    "    # Suponiendo que gdf1 y gdf2 son tus GeoDataFrames de puntos\n",
    "    gdf1_coords = gdf1.geometry.apply(lambda geom: (geom.x, geom.y)).tolist()\n",
    "    gdf2_coords = gdf2.geometry.apply(lambda geom: (geom.x, geom.y)).tolist()\n",
    "\n",
    "    # Crear árboles para ambas capas\n",
    "    tree_gdf1 = cKDTree(gdf1_coords)\n",
    "    tree_gdf2 = cKDTree(gdf2_coords)\n",
    "\n",
    "    # Buscar los puntos más cercanos en gdf2 para cada punto en gdf1\n",
    "    distances, indices = tree_gdf1.query(gdf2_coords, k=1)\n",
    "\n",
    "    # Filtrar índices que estén fuera de los límites\n",
    "    valid_indices = [i for i in range(len(indices)) if indices[i] < len(gdf1)]\n",
    "\n",
    "    # Crear un DataFrame con resultados válidos\n",
    "    results = pd.DataFrame({\n",
    "        'gdf1_index': [indices[i] for i in valid_indices],\n",
    "        'gdf2_index': valid_indices,\n",
    "        'distance': [distances[i] for i in valid_indices]\n",
    "    })\n",
    "\n",
    "    # Verificar que no haya duplicados en la relación 1 a 1\n",
    "    duplicated_gdf1 = results.duplicated(subset='gdf1_index', keep=False)\n",
    "    duplicated_gdf2 = results.duplicated(subset='gdf2_index', keep=False)\n",
    "\n",
    "    if not duplicated_gdf1.any() and not duplicated_gdf2.any():\n",
    "        print(\"Relación 1 a 1 verificada\")\n",
    "    else:\n",
    "        print(\"Se encontraron duplicados, la relación no es 1 a 1\")\n",
    "\n",
    "    # Unir los GeoDataFrames usando los índices válidos obtenidos\n",
    "    gdf1_matched = gdf1.iloc[results['gdf1_index']].reset_index(drop=True)\n",
    "    gdf2_matched = gdf2.iloc[results['gdf2_index']].reset_index(drop=True)\n",
    "\n",
    "    # Resultado unido\n",
    "    return gdf1_matched.join(gdf2_matched, lsuffix='', rsuffix='_gdf2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_300 = join_by_nearest(df_300,btw)#.drop('geometry_gdf2')\n",
    "df_600 = join_by_nearest(df_600,btw)\n",
    "df_900 = join_by_nearest(df_900,btw)\n",
    "df_1200 = join_by_nearest(df_1200,btw)\n",
    "df_1500 = join_by_nearest(df_1500,btw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_300 = df_300.set_crs(25830)\n",
    "df_600 = df_600.set_crs(25830)\n",
    "df_900 = df_900.set_crs(25830)\n",
    "df_1200 = df_1200.set_crs(25830)\n",
    "df_1500 = df_1500.set_crs(25830)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_900.drop(columns=['geometry_gdf2']).to_file(r\"G:\\Mi unidad\\walknet_datalake\\temp\\df_900.gpkg\",driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_tzs(df):\n",
    "    df = df[df['local_population']!=0]\n",
    "    # Excluir las columnas no deseadas al definir las columnas para el promedio ponderado\n",
    "    columns_to_average = [col for col in df.columns if col not in ['geometry', 'node_id', 'tz_id', 'local_population', 'geometry_gdf2', 'hex_id', 'id', 'left', 'top', 'right', 'bottom', 'row_index', 'col_index']]\n",
    "    # Función para calcular el promedio ponderado\n",
    "    def weighted_average(dfg):\n",
    "        weights = dfg['local_population']\n",
    "        return pd.Series({\n",
    "            col: (dfg[col] * weights).sum() / weights.sum() for col in columns_to_average\n",
    "        })\n",
    "    \n",
    "    # Agrupar por el ID de hexágono y calcular el promedio ponderado para cada columna\n",
    "    df_aggregated = df.groupby('tz_id')[['local_population']+columns_to_average].apply(weighted_average).reset_index()\n",
    "    \n",
    "    return df_aggregated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def agg_tzs(df):\n",
    "    df = df[df['local_population'] != 0]\n",
    "    # Exclude unwanted columns from the weighted average calculation\n",
    "    columns_to_average = [\n",
    "        col for col in df.columns if col not in [\n",
    "            'geometry', 'node_id', 'tz_id', 'local_population', \n",
    "            'geometry_gdf2', 'hex_id', 'id', 'left', 'top', \n",
    "            'right', 'bottom', 'row_index', 'col_index'\n",
    "        ]\n",
    "    ]\n",
    "    \n",
    "    # Function to calculate the weighted average for each column\n",
    "    def weighted_average(dfg):\n",
    "        weights = dfg['local_population']\n",
    "        result = {}\n",
    "        for col in columns_to_average:\n",
    "            # Filter out rows with NaN values in the current column\n",
    "            valid_rows = dfg[~dfg[col].isna()]\n",
    "            if valid_rows.empty:\n",
    "                result[col] = None  # Assign None if no valid rows are left\n",
    "            else:\n",
    "                weights_filtered = valid_rows['local_population']\n",
    "                result[col] = (valid_rows[col] * weights_filtered).sum() / weights_filtered.sum()\n",
    "        return pd.Series(result)\n",
    "    \n",
    "    # Group by tz_id and calculate the weighted average for each column\n",
    "    df_aggregated = df.groupby('tz_id')[['local_population'] + columns_to_average].apply(weighted_average).reset_index()\n",
    "    \n",
    "    return df_aggregated\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tz_300 = agg_tzs(df_300)\n",
    "df_tz_600 = agg_tzs(df_600)\n",
    "df_tz_900 = agg_tzs(df_900)\n",
    "df_tz_1200 = agg_tzs(df_1200)\n",
    "df_tz_1500 = agg_tzs(df_1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tz_900[df_tz_900['des_building_age'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tz_300[df_tz_300['des_building_age'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tz_1500['des_building_age'] = df_tz_1500['des_building_age'].fillna(0).astype(int)\n",
    "df_tz_1200['des_building_age'] = df_tz_1200['des_building_age'].fillna(0).astype(int)\n",
    "df_tz_900['des_building_age'] = df_tz_900['des_building_age'].fillna(0).astype(int)\n",
    "df_tz_600['des_building_age'] = df_tz_600['des_building_age'].fillna(0).astype(int)\n",
    "df_tz_300['des_building_age'] = df_tz_300['des_building_age'].fillna(0).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tz_900['des_building_age'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [1880, 1930, 1960, 1980, 2000, 2010, 2018]\n",
    "labels = ['1880-1930', '1930-1960', '1960-1980', '1980-2000', '2000-2010', '2010-2018']\n",
    "\n",
    "# Use pd.cut to bucketize\n",
    "df_tz_300['des_age_range'] = pd.cut(df_tz_300['des_building_age'], bins=bins, labels=labels, right=False)\n",
    "df_tz_600['des_age_range'] = pd.cut(df_tz_600['des_building_age'], bins=bins, labels=labels, right=False)\n",
    "df_tz_900['des_age_range'] = pd.cut(df_tz_900['des_building_age'], bins=bins, labels=labels, right=False)\n",
    "df_tz_1200['des_age_range'] = pd.cut(df_tz_1200['des_building_age'], bins=bins, labels=labels, right=False)\n",
    "df_tz_1500['des_age_range'] = pd.cut(df_tz_1500['des_building_age'], bins=bins, labels=labels, right=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['den_hou_Q1','den_hou_Q2','den_hou_Q3','den_hou_Q4']\n",
    "\n",
    "# Use pd.cut to bucketize\n",
    "df_tz_300['den_quartile'] = pd.qcut(df_tz_300['dens_hou_total'], 4, labels=labels)\n",
    "df_tz_600['den_quartile'] = pd.qcut(df_tz_600['dens_hou_total'], 4, labels=labels)\n",
    "df_tz_900['den_quartile'] = pd.qcut(df_tz_900['dens_hou_total'], 4, labels=labels)\n",
    "df_tz_1200['den_quartile'] = pd.qcut(df_tz_1200['dens_hou_total'], 4, labels=labels)\n",
    "df_tz_1500['den_quartile'] = pd.qcut(df_tz_1500['dens_hou_total'], 4, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_1500 = pd.merge(df, df_tz_1500, left_on='id_taz',right_on=\"tz_id\",how='left')\n",
    "ds_1200 = pd.merge(df, df_tz_1200, left_on='id_taz',right_on=\"tz_id\",how='left')\n",
    "ds_900 = pd.merge(df, df_tz_900, left_on='id_taz',right_on=\"tz_id\",how='left')\n",
    "ds_600 = pd.merge(df, df_tz_600, left_on='id_taz',right_on=\"tz_id\",how='left')\n",
    "ds_300 = pd.merge(df, df_tz_300, left_on='id_taz',right_on=\"tz_id\",how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testlogit = ds_1500[['elevator','trip_purpose', 'trip_distance', 'trip_mode', 'dem_gender', 'dem_income','dem_age','dem_education', 'dem_activity', 'dem_hou_size','acc_shopping_market', 'acc_shopping_alone']]\n",
    "df_testlogit = df_testlogit[df_testlogit['dem_income'].notnull()]\n",
    "for v in ['acc_shopping_market', 'acc_shopping_alone']:\n",
    "    df_testlogit[v] = df_testlogit[v].fillna(0)\n",
    "df_testlogit['acc_shopping'] = df_testlogit['acc_shopping_alone'] + df_testlogit['acc_shopping_market']\n",
    "df_testlogit = df_testlogit.drop(columns = ['acc_shopping_market', 'acc_shopping_alone'])\n",
    "df_testlogit = df_testlogit[df_testlogit['trip_purpose'] == 'Shopping']\n",
    "df_testlogit['dem_education'] = df_testlogit['dem_education'].map({'Second - B':3, 'First':1, 'Third':4, 'Second - A':2, 'No':0})\n",
    "df_testlogit['dem_activity'] = df_testlogit['dem_activity'].map({'Retired':0, 'Caretaker':1, 'Unemployed':0, 'Worker':1, 'Student':1})\n",
    "df_testlogit['dem_gender'] = df_testlogit['dem_gender'].map({'Female':0,'Male':1})\n",
    "df_testlogit = df_testlogit[df_testlogit['trip_mode'] != 'Other']\n",
    "df_testlogit['trip_mode'] = df_testlogit['trip_mode'].map({'Walking':'active', 'Motor - Personal':'driving', 'Bus':'transit', 'Cycling':'active', 'Train':'transit','Motor - Shared':'driving'})\n",
    "df_testlogit = df_testlogit[(df_testlogit['dem_age']>=18)&(df_testlogit['dem_age']<=65)]\n",
    "df_testlogit = df_testlogit[df_testlogit['trip_mode'].isin(['active','driving'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testlogit.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testlogit.to_csv(r\"C:\\Users\\katul\\OneDrive\\Escritorio\\df_testlogit.csv\",index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "csfont = {'fontname':'Adobe Garamond Pro','fontsize':30}\n",
    "hfont = {'fontname':'Adobe Garamond Pro','fontsize':15}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Filter dataset for valid rows\n",
    "data_filtered = df_testlogit[(df_testlogit['dem_age'] >= 18) & (df_testlogit['dem_age'] <= 65)]\n",
    "# Step 2: Balance the `trip_mode` column\n",
    "active = data_filtered[data_filtered['trip_mode'] == 'active']\n",
    "driving = data_filtered[data_filtered['trip_mode'] == 'driving']\n",
    "# Resample the minority class to balance the dataset\n",
    "min_class_count = min(len(active), len(driving))\n",
    "active_resampled = resample(active, replace=False, n_samples=min_class_count, random_state=42)\n",
    "driving_resampled = resample(driving, replace=False, n_samples=min_class_count, random_state=42)\n",
    "# Combine the balanced dataset\n",
    "data_filtered = pd.concat([active_resampled, driving_resampled])\n",
    "\n",
    "# Step 1: Define features and target variable\n",
    "features = ['trip_distance', 'dem_gender', 'dem_income', 'dem_age','dem_education', 'dem_activity', 'dem_hou_size', 'acc_shopping']\n",
    "target = 'trip_mode'\n",
    "# Encode the target variable: 'active' -> 0, 'driving' -> 1\n",
    "data_filtered['trip_mode_encoded'] = data_filtered['trip_mode'].map({'active': 0, 'driving': 1})\n",
    "# Step 2: Scale numeric features\n",
    "scaler = StandardScaler()\n",
    "data_filtered[features] = scaler.fit_transform(data_filtered[features])\n",
    "# Step 3: Split into training and testing sets\n",
    "X = data_filtered[features]\n",
    "y = data_filtered['trip_mode_encoded']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Display the shape of the training and testing sets\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
    "\n",
    "# Add a constant term for the intercept in the model\n",
    "X_train_const = sm.add_constant(X_train)\n",
    "\n",
    "# Fit the Binomial Logit Model using the training data and weights\n",
    "logit_model = sm.Logit(y_train, X_train_const)\n",
    "result = logit_model.fit()\n",
    "\n",
    "# Display the summary of the model\n",
    "result.summary()\n",
    "\n",
    "# Prepare test data with a constant term\n",
    "X_test_const = sm.add_constant(X_test)\n",
    "\n",
    "# Predict probabilities for the test set\n",
    "y_pred_prob = result.predict(X_test_const)\n",
    "\n",
    "# Convert probabilities to binary predictions using 0.5 as threshold\n",
    "y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "\n",
    "# Evaluate model performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "# Plot ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})',color='r')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random Guess')\n",
    "plt.xlabel('False Positive Rate',**hfont)\n",
    "plt.ylabel('True Positive Rate',**hfont)\n",
    "plt.title('ROC Curve No Distance Threshold',**csfont)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig(r\"C:\\Users\\katul\\OneDrive\\Escritorio\\ROC_Binary Logit Model.jpg\")\n",
    "plt.show()\n",
    "\n",
    "accuracy, roc_auc\n",
    "\n",
    "# Extract coefficients and their confidence intervals\n",
    "coefficients = result.params\n",
    "conf_intervals = result.conf_int()\n",
    "features_with_const = ['Intercept'] + features  # Include the constant term\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "impact_df = pd.DataFrame({\n",
    "    'Feature': features_with_const,\n",
    "    'Coefficient': coefficients.values,\n",
    "    'Lower CI': conf_intervals[0].values,\n",
    "    'Upper CI': conf_intervals[1].values\n",
    "}).set_index('Feature')\n",
    "\n",
    "# Sort by absolute coefficient values for better visualization\n",
    "impact_df['Abs Coefficient'] = np.abs(impact_df['Coefficient'])\n",
    "impact_df = impact_df.sort_values(by='Abs Coefficient', ascending=False)\n",
    "\n",
    "# Plot feature impacts with confidence intervals\n",
    "plt.figure(figsize=(8.5, 6))\n",
    "plt.errorbar(\n",
    "    impact_df.index, impact_df['Coefficient'], \n",
    "    yerr=(impact_df['Coefficient'] - impact_df['Lower CI'], impact_df['Upper CI'] - impact_df['Coefficient']),\n",
    "    fmt='o', capsize=5, label=\"Coefficient ± CI\",color='r'\n",
    ")\n",
    "plt.axhline(0, color='gray', linestyle='--', linewidth=0.8)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Feature Impact on Mode Choice\\nNo Distance Threshold',**csfont)\n",
    "plt.ylabel('Coefficient',**hfont)\n",
    "plt.xlabel('Feature',**hfont)\n",
    "plt.tight_layout()\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(r\"C:\\Users\\katul\\OneDrive\\Escritorio\\FeatureImpact Logit Model.jpg\")\n",
    "plt.show()\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Add a constant term for the intercept\n",
    "X_train_const = sm.add_constant(X_train)\n",
    "\n",
    "# Fit the Binomial Logit Model\n",
    "logit_model = sm.Logit(y_train, X_train_const)\n",
    "result = logit_model.fit()\n",
    "\n",
    "# Summary and ROC Curve\n",
    "print(result.summary())\n",
    "y_pred_prob = result.predict(sm.add_constant(X_test))\n",
    "#plot_roc_curve(y_test, y_pred_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILTER AGAIN FOR DISTANCE IN PROXIMITY\n",
    "\n",
    "# Step 1: Filter dataset for valid rows\n",
    "data_filtered = df_testlogit[(df_testlogit['dem_age'] >= 18) & (df_testlogit['dem_age'] <= 65)]\n",
    "data_filtered = data_filtered[data_filtered['trip_distance']<1.5]\n",
    "# Step 2: Balance the `trip_mode` column\n",
    "active = data_filtered[data_filtered['trip_mode'] == 'active']\n",
    "driving = data_filtered[data_filtered['trip_mode'] == 'driving']\n",
    "# Resample the minority class to balance the dataset\n",
    "min_class_count = min(len(active), len(driving))\n",
    "active_resampled = resample(active, replace=False, n_samples=min_class_count, random_state=42)\n",
    "driving_resampled = resample(driving, replace=False, n_samples=min_class_count, random_state=42)\n",
    "# Combine the balanced dataset\n",
    "data_filtered = pd.concat([active_resampled, driving_resampled])\n",
    "\n",
    "# Step 1: Define features and target variable\n",
    "features = ['trip_distance', 'dem_gender', 'dem_income', 'dem_age','dem_education', 'dem_activity', 'dem_hou_size', 'acc_shopping']\n",
    "target = 'trip_mode'\n",
    "# Encode the target variable: 'active' -> 0, 'driving' -> 1\n",
    "data_filtered['trip_mode_encoded'] = data_filtered['trip_mode'].map({'active': 0, 'driving': 1})\n",
    "# Step 2: Scale numeric features\n",
    "scaler = StandardScaler()\n",
    "data_filtered[features] = scaler.fit_transform(data_filtered[features])\n",
    "# Step 3: Split into training and testing sets\n",
    "X = data_filtered[features]\n",
    "y = data_filtered['trip_mode_encoded']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Display the shape of the training and testing sets\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
    "\n",
    "# Add a constant term for the intercept in the model\n",
    "X_train_const = sm.add_constant(X_train)\n",
    "\n",
    "# Fit the Binomial Logit Model using the training data and weights\n",
    "logit_model = sm.Logit(y_train, X_train_const)\n",
    "result = logit_model.fit()\n",
    "\n",
    "# Display the summary of the model\n",
    "result.summary()\n",
    "\n",
    "# Prepare test data with a constant term\n",
    "X_test_const = sm.add_constant(X_test)\n",
    "\n",
    "# Predict probabilities for the test set\n",
    "y_pred_prob = result.predict(X_test_const)\n",
    "\n",
    "# Convert probabilities to binary predictions using 0.5 as threshold\n",
    "y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "\n",
    "# Evaluate model performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "# Plot ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})',color='r')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random Guess')\n",
    "plt.xlabel('False Positive Rate',**hfont)\n",
    "plt.ylabel('True Positive Rate',**hfont)\n",
    "plt.title('ROC Curve 1500 m',**csfont)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig(r\"C:\\Users\\katul\\OneDrive\\Escritorio\\ROC_Binary Logit Model Proximity.jpg\")\n",
    "plt.show()\n",
    "\n",
    "print(accuracy, roc_auc)\n",
    "\n",
    "# Extract coefficients and their confidence intervals\n",
    "coefficients = result.params\n",
    "conf_intervals = result.conf_int()\n",
    "features_with_const = ['Intercept'] + features  # Include the constant term\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "impact_df = pd.DataFrame({\n",
    "    'Feature': features_with_const,\n",
    "    'Coefficient': coefficients.values,\n",
    "    'Lower CI': conf_intervals[0].values,\n",
    "    'Upper CI': conf_intervals[1].values\n",
    "}).set_index('Feature')\n",
    "\n",
    "# Sort by absolute coefficient values for better visualization\n",
    "impact_df['Abs Coefficient'] = np.abs(impact_df['Coefficient'])\n",
    "impact_df = impact_df.sort_values(by='Abs Coefficient', ascending=False)\n",
    "\n",
    "# Plot feature impacts with confidence intervals\n",
    "plt.figure(figsize=(8.5, 6))\n",
    "plt.errorbar(\n",
    "    impact_df.index, impact_df['Coefficient'], \n",
    "    yerr=(impact_df['Coefficient'] - impact_df['Lower CI'], impact_df['Upper CI'] - impact_df['Coefficient']),\n",
    "    fmt='o', capsize=5, label=\"Coefficient ± CI\",color='r'\n",
    ")\n",
    "plt.axhline(0, color='gray', linestyle='--', linewidth=0.8)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Feature Impact on Mode Choice\\n1500 m Threshold',**csfont)\n",
    "plt.ylabel('Coefficient',**hfont)\n",
    "plt.xlabel('Feature',**hfont)\n",
    "plt.tight_layout()\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(r\"C:\\Users\\katul\\OneDrive\\Escritorio\\FeatureImpact Logit Model Proximity.jpg\")\n",
    "plt.show()\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Add a constant term for the intercept\n",
    "X_train_const = sm.add_constant(X_train)\n",
    "\n",
    "# Fit the Binomial Logit Model\n",
    "logit_model = sm.Logit(y_train, X_train_const)\n",
    "result = logit_model.fit()\n",
    "\n",
    "# Summary and ROC Curve\n",
    "print(result.summary())\n",
    "y_pred_prob = result.predict(sm.add_constant(X_test))\n",
    "#plot_roc_curve(y_test, y_pred_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "lists = {\n",
    "        'trip_purpose' : ds_900.trip_purpose.unique(),\n",
    "        'dem_cohort' : ds_900.dem_cohort.unique()}\n",
    "\n",
    "lists_with_all = {key: list(values) + [\"all\"] for key, values in lists.items()}\n",
    "\n",
    "# Generate all combinations of the lists\n",
    "combinations = product(*lists_with_all.values())\n",
    "\n",
    "# Build dictionaries for each combination\n",
    "dict_combinations = [dict(zip(lists_with_all.keys(), combination)) for combination in combinations]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_combinations  = {\"|\".join([str(e) for e in di.values()]):di for di in dict_combinations}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter DataFrame based on a dictionary\n",
    "def filter_dataframe(df, filter_dict):\n",
    "    filtered_df = df.copy()\n",
    "    for key, value in filter_dict.items():\n",
    "        if value != \"all\":\n",
    "            filtered_df = filtered_df[filtered_df[key] == value]\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one DataFrame per combination\n",
    "target_dataframes = {'300':{},'600':{},'1200':{},'900':{},'1500':{}}\n",
    "for dft in ('300',ds_300),('600',ds_600),('900',ds_900),('1200',ds_1200),('1500',ds_1500):\n",
    "    for name, combo in control_combinations.items():\n",
    "        filtered_df = filter_dataframe(dft[1], combo)\n",
    "        target_dataframes[dft[0]].update({name: filtered_df})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the filtered DataFrames (or use them as needed)\n",
    "i = 0\n",
    "for dist, dat in target_dataframes.items():\n",
    "    for name, filtered_df in dat.items():\n",
    "        if not filtered_df.empty:\n",
    "            i+= 1\n",
    "            print(f\"Distance threshold {dist}\")\n",
    "            print(f\"Combination: {name}\")\n",
    "            print(str(len(filtered_df)))\n",
    "            print(str(filtered_df['elevator'].sum()))\n",
    "            print(\"-\" * 40)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_metrics = {}\n",
    "for dist, dfs in target_dataframes.items():\n",
    "    for name, df in dfs.items():\n",
    "        if df['elevator'].sum() > 45000:\n",
    "            table_size = len(df)\n",
    "            sample_size = df['elevator'].sum()\n",
    "            sample_walk = df[df['trip_mode']=='Walking']['elevator'].sum()\n",
    "            sample_drive = df[df['trip_mode']=='Motor - Personal']['elevator'].sum()\n",
    "            walk_perc = round((100 * sample_walk) / sample_size,0)\n",
    "            drive_perc = round((100 * sample_drive) / sample_size,2)\n",
    "            sample_metrics.update({f\"{dist}|{name}\":{'table_size': table_size,'sample_size': sample_size,'sample_walk': sample_walk,'sample_drive': sample_drive, 'walk_perc':walk_perc, 'drive_perc':drive_perc}})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_metrics = pd.DataFrame.from_dict(sample_metrics, orient='index').fillna(0)\n",
    "sample_metrics['sample_size'] = sample_metrics.sample_size.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_metrics.reset_index(inplace=True)\n",
    "sample_metrics = sample_metrics[sample_metrics['index'].str.contains('1500')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Sort the data by sample_size in descending order\n",
    "\n",
    "df_sorted = sample_metrics.copy()\n",
    "df_sorted.set_index('index',inplace=True)\n",
    "\n",
    "df_sorted['walk_size'] = df_sorted['sample_walk'] \n",
    "df_sorted['drive_size'] = df_sorted['sample_drive']\n",
    "\n",
    "# Ensure proper sorting and data handling\n",
    "df_sorted = df_sorted.sort_values(by='sample_size', ascending=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8.5, 11))\n",
    "fig.suptitle('Sample Size by Control Filters', fontweight=\"bold\", y=1, **csfont)\n",
    "\n",
    "cr = {\n",
    "    'walk_size': 'red',\n",
    "    'drive_size': 'black'}\n",
    "\n",
    "df_sorted[df_sorted.index != '1500|all|all'][['walk_size','drive_size']].rename(columns={'walk_size':\"Walking\",'drive_size':\"Driving\"}).plot(kind='barh',stacked=True,ax=ax, color=['red','black']) \n",
    "\n",
    "\n",
    "for i, (index, row) in enumerate(df_sorted[df_sorted.index != '900|all|all'].iterrows()):\n",
    "    # Calculate the total length of the stacked bar\n",
    "    total_length = row['walk_size'] + row['drive_size']\n",
    "    \n",
    "    # Create the label text in the format \"{walk_perc} / {drive_perc}\"\n",
    "    label_text = f\"{row['walk_perc']:.0f} / {row['drive_perc']:.0f} %\"\n",
    "    \n",
    "    # Place the label at the end of the bar\n",
    "    ax.text(\n",
    "        x=total_length + 5000,  # Slightly offset to the right of the bar\n",
    "        y=i,  # Corresponds to the bar index\n",
    "        s=label_text,\n",
    "        va='center',\n",
    "        ha='left',\n",
    "        color='black',\n",
    "        fontsize=9\n",
    "    )\n",
    "ax.set_yticklabels(\n",
    "    df_sorted[df_sorted.index != '1500|all|all'].index,  # Use the index of the DataFrame for labels\n",
    "    fontdict=hfont\n",
    ")\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.grid(axis='x', linestyle='--', color='gray', alpha=0.5) \n",
    "ax.set_ylabel('')\n",
    "ax.ticklabel_format(axis='x', style='plain', useOffset=False)  # No scientific notation\n",
    "plt.xticks(rotation=35)\n",
    "plt.tight_layout()\n",
    "#plt.savefig(r\"C:\\Users\\katul\\OneDrive\\Escritorio\\dataset_balance.jpg\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dft in ('300',ds_300),('600',ds_600),('900',ds_900),('1200',ds_1200),('1500',ds_1500):\n",
    "    dft[1].to_parquet(r\"G:\\Mi unidad\\walknet_datalake\\extractions\\extraction_{di}.parquet\".format(di = dft[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
